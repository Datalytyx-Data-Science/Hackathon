{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case - predicting the demand for items at a libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfOptions = {\n",
    "#   \"sfURL\" : \"datalytyx.east-us-2.azure.snowflakecomputing.com\",\n",
    "#   \"sfAccount\" : \"datalytyx\",\n",
    "#   \"sfUser\" : \"WILLHOLTAM\",\n",
    "#   \"sfPassword\" : \"04MucSfLV\",\n",
    "#   \"sfRole\": \"DATABRICKS\",\n",
    "#   \"sfDatabase\" : \"DATABRICKS_DEMO\",\n",
    "#   \"sfSchema\" : \"SEATTLE_LIBRARY\",\n",
    "#   \"sfWarehouse\" : \"DATASCIENCE_WH\"\n",
    "# }\n",
    "# SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #spark.conf.set(\"spark.executor.cores\",2)\n",
    "\n",
    "# df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "#   .options(**sfOptions) \\\n",
    "#   .option(\"query\", \"\"\"select * from library_collection_inventory where reportdate in ('2017-09-01T00:00:00','2017-10-01T00:00:00', '2017-11-01T00:00:00', '2017-12-01T00:00:00', '2018-01-01T00:00:00', '2018-01-01T00:00:00', '2018-02-01T00:00:00', '2018-02-01T00:00:00', '2018-03-01T00:00:00', '2018-04-01T00:00:00', '2018-05-01T00:00:00', '2018-06-01T00:00:00', '2018-07-01T00:00:00') \"\"\") \\\n",
    "#   .load().limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a view or table\n",
    "\n",
    "# temp_table_name = \"library_collection_inventory\"\n",
    "\n",
    "# df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk  # Has to be added through Workspaces/ attach library to cluster\n",
    "import more_itertools\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import mpld3\n",
    "from snowflake.sqlalchemy import URL\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = df.toPandas()  # Create pandas dataframe to work within python when in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_account = \"datalytyx\"\n",
    "sf_user = \"WILLHOLTAM\"\n",
    "sf_pwd = \"04MucSfLV\"\n",
    "# sf_user = \"CHRISSCHON\"\n",
    "# sf_pwd = \"UpsetSheep7\"\n",
    "sf_role = \"DATABRICKS\"\n",
    "sf_db = \"DATABRICKS_DEMO\"\n",
    "sf_schema = \"SEATTLE_LIBRARY\"\n",
    "sf_wh = \"DATASCIENCE_WH\"\n",
    "sf_region = \"east-us-2.azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URL(\n",
    "    user = sf_user,\n",
    "    password = sf_pwd,\n",
    "    account = sf_account,\n",
    "    region = sf_region,\n",
    "    database = sf_db,\n",
    "    schema = sf_schema,\n",
    "    warehouse = sf_wh,\n",
    "    role = sf_role,\n",
    "))\n",
    "\n",
    "# engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_sql_query(\"select * from library_collection_inventory where reportdate in ('2017-09-01T00:00:00','2017-10-01T00:00:00', '2017-11-01T00:00:00', '2017-12-01T00:00:00', '2018-01-01T00:00:00', '2018-01-01T00:00:00', '2018-02-01T00:00:00', '2018-02-01T00:00:00', '2018-03-01T00:00:00', '2018-04-01T00:00:00', '2018-05-01T00:00:00', '2018-06-01T00:00:00', '2018-07-01T00:00:00') limit 1000\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WilliamHoltam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WilliamHoltam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Common words to ignore\n",
    "nltk.download('punkt')  # Punkt Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.dropna(axis=1, how='all')  # Drop the columns where all of the elements are missing values\n",
    "# df_pandas.dropna(axis=0, how='any')  # Drop the rows where any of the elements are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoneReplacer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer changes Nonetype values into numpy NaN values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "#        X_fitted = X.where(X == None)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        #[\"\" if item is None else str(item) for item in X.select_dtypes(include='object')]\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        X.fillna(value = pd.np.nan, inplace=True)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = NoneReplacer()\n",
    "\n",
    "initiated_class.fit(features)\n",
    "\n",
    "df_pandas_fitter = initiated_class.transform(features)\n",
    "# df_pandas_fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyColumnRemover(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer drops empty columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):  # has to take an optional y for pipelines\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the number of missing values that corresponds to the threshold.\n",
    "        Detects and labels columns with equal to or greater than numbers of missing values than the threshold.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.drop_columns = features.isna().sum()[features.isna().sum() >= X.shape[0]].index  # Calculates pd.series with column lables as indecies\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Drops columns containing empty values.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        return X.drop(columns = self.drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = EmptyColumnRemover()\n",
    "\n",
    "initiated_class.fit(X=df_pandas_fitter)\n",
    "\n",
    "df_pandas_fitter1 = initiated_class.transform(df_pandas_fitter)\n",
    "# df_pandas_fitter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyNaNRowRemover(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y = None):  # has to take an optional y for pipelines\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the number of missing values the corresponds to the threshold.\n",
    "        Detects and labels columns with more missing values that the threshold. \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        return X.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = AnyNaNRowRemover()\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter1)\n",
    "\n",
    "df_pandas_fitter2 = initiated_class.transform(df_pandas_fitter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeAndStemer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, default_column = 'subjects'):\n",
    "        self.default_column = default_column\n",
    "        pass\n",
    "    \n",
    "    def tokenize_and_stem(self, X):\n",
    "        \n",
    "        tokens = [word for sent in nltk.sent_tokenize(X) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "        \n",
    "        stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "        return stems\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        totalvocab_stemmed = []\n",
    "        \n",
    "        allwords_stemmed = [self.tokenize_and_stem(str(i)) for i in X[self.default_column].tolist()] #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed)\n",
    "        totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed))\n",
    "        \n",
    "        return totalvocab_stemmed\n",
    "\n",
    "# How to implement in class above???? OR do I have to implement it in a separate class...  \n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = TokenizeAndStemer()\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter2)\n",
    "\n",
    "df_pandas_fitter3 = initiated_class.transform(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeOnly(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, default_column = 'subjects'):\n",
    "        self.default_column = default_column\n",
    "        pass\n",
    "\n",
    "    def tokenize_only(self, X):\n",
    "        \n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "        tokens = [word.lower() for sent in nltk.sent_tokenize(X) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        \n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "                \n",
    "        return filtered_tokens\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        totalvocab_tokenized = []\n",
    "    \n",
    "        allwords_tokenized = [self.tokenize_only(str(i)) for i in X[self.default_column].tolist()] #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized))\n",
    "        \n",
    "        return totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = TokenizeOnly()\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter2)\n",
    "\n",
    "df_pandas_fitter3 = initiated_class.transform(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19114,)\n"
     ]
    }
   ],
   "source": [
    "union = FeatureUnion([(\"tas\", TokenizeAndStemer()),\n",
    "                      (\"to\", TokenizeOnly())])\n",
    "\n",
    "test = union.fit_transform(df_pandas_fitter2)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenizeOnly(TransformerMixin, BaseEstimator):\n",
    "\n",
    "#     def __init__(self, default_column = 'subjects'):\n",
    "#         self.default_column = default_column\n",
    "#         pass\n",
    "    \n",
    "#      def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "# print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_pipeline = Pipeline([\n",
    "    ('nr', NoneReplacer()),\n",
    "    ('ecr', EmptyColumnRemover()),\n",
    "    ('anrr', AnyNaNRowRemover())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pre_processing_pipeline.fit_transform(X = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_stem_pipeline = Pipeline([\n",
    "    ('tas', TokenizeAndStemer())\n",
    "])\n",
    "\n",
    "list_tokenized_and_stemmed = tokenize_and_stem_pipeline.fit_transform(X = df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_only_pipeline = Pipeline([   \n",
    "    ('to', TokenizeOnly())\n",
    "])\n",
    "\n",
    "list_tokenized = tokenize_only_pipeline.fit_transform(X = df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(X, default_column = 'subjects'):\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(X[default_column]) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "#     filtered_tokens = []\n",
    "#     [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "    \n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "#     return stems\n",
    "\n",
    "# tokenize_and_stem(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "# tokenize_and_stem(df_pandas_fitter2['subjects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = df_pandas['subjects']\n",
    "\n",
    "# Define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "# #class TextTransform()\n",
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "\n",
    "# def tokenize_only(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     return filtered_tokens\n",
    "\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "\n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) \n",
    "\n",
    "# allwords_tokenized = [tokenize_only(str(i)) for i in subjects]\n",
    "# totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in subjects_c:\n",
    "#     allwords_stemmed = tokenize_and_stem(str(i)) #for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(str(i))\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return pd.DataFrame(self.model.predict(X))\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('cluster', ModelTransformer(KMeans_foo(3))),\n",
    "#     ('binarize', LabelBinarizer())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.loc[:,'PUBLICATIONYEAR'] = df_pandas.loc[:,'PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n",
    "# _______________________________________________________________________________\n",
    "# Functions for transforming the data\n",
    "# def processing(df):\n",
    "#     df['PUBLICATIONYEAR'] = df['PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "#     [str(item) for item in df['SUBJECTS'] if item is None]\n",
    "\n",
    "# processing(df_pandas)\n",
    "    \n",
    "    # def ext_date_fun(input, output):\n",
    "#   output = input.str.extractstr.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "  \n",
    "# def fix_none_fun(sub):\n",
    "#   [str(item) for item in sub if item is None]\n",
    "# _______________________________________________________________________________\n",
    "\n",
    "# # Attempt at putting the functions into classes\n",
    "# class PublicationYearCleaner(object):\n",
    "#   \"\"\"Preprocessing: This class cleans the Publication Year Column\"\"\"\n",
    "#   def __init__(self, data):\n",
    "#     self.raw = data\n",
    "  \n",
    "#   def ext_date(self, pub_year):\n",
    "#      pub_year = pub_year.str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n",
    "# class SubjectsCleaner(objct)\n",
    "#   def fix_none(self, string):\n",
    "#     if string is None:\n",
    "#       return ''\n",
    "#     return str(string)\n",
    "# # ________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.loc[:,'PUBLICATIONYEAR'] = df_pandas.loc[:,'PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000  # Number of rows to analyse\n",
    "\n",
    "# titles = df_pandas.loc[0:n,'TITLE'].values.tolist()\n",
    "# subjects = df_pandas.loc[0:n,'SUBJECTS'].values.tolist()\n",
    "# author = df_pandas.loc[0:n, 'AUTHOR'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return pd.DataFrame(self.model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMTYPE']), how='inner')\n",
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMCOLLECTION']), how='inner')\n",
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMLOCATION']), how='inner')\n",
    "# list(df_pandas.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use extend so it's a big flat list of vocab\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in df_pandas['subjects_c']:\n",
    "#     allwords_stemmed = tokenize_and_stem(str(i)) #for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(str(i))\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 13598 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              words\n",
      "allon         allon\n",
      "gabriel     gabriel\n",
      "fictiti  fictitious\n",
      "charact   character\n",
      "fiction     fiction\n"
     ]
    }
   ],
   "source": [
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 450 ms\n",
      "(1000, 737)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "vectorizer_pipe = Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=5, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3)))])\n",
    "\n",
    "%time tfidf_matrix = vectorizer_pipe.fit_transform(subjects)  # fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-838-d2a0c894f183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcluster_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3425\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cluster_pipe.fit(tfidf_matrix)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "dist = 1 - euclidean_distances(tfidf_matrix)\n",
    "\n",
    "cluster_pipe = Pipeline(steps=[('cluster', KMeans(n_clusters = 5, random_state=3425))])\n",
    "%time cluster_pipe.fit(tfidf_matrix)\n",
    "clusters = cluster_pipe.named_steps['cluster'].labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(cluster_pipe,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = cluster_pipe.named_steps['cluster'].labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = { 'title': titles, 'author': author, 'subjects': subjects_c, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(books, index = [clusters] , columns = ['title', 'author', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame.columns\n",
    "frame.cluster.value_counts() #number of books per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(ind)\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = cluster_pipe.named_steps['cluster'].cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace\n",
    "    \n",
    "    for ind in order_centroids[i, :3]: #replace 6 with n words per cluster\n",
    "        x += ' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=','\n",
    "    \n",
    "#    print(\"Cluster %d titles:\" % i, end='')\n",
    "#    for title in frame.loc[i,'title'].values.tolist():\n",
    "#        print(' %s,' % title, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os  # for os.path.basename\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# from sklearn.manifold import MDS\n",
    "\n",
    "# MDS()\n",
    "\n",
    "# # convert two components as we're plotting points in a two-dimensional plane\n",
    "# # \"precomputed\" because we provide a distance matrix\n",
    "# # we will also specify `random_state` so the plot is reproducible.\n",
    "# mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "# pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "# xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(frame.cluster)\n",
    "dummies.columns = dummies.columns.astype(str)\n",
    "list(dummies.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies.rename(index=str, columns={\"0\": \"Juvenile Literature\", \"1\": \"Music (Country)\", \"2\": \"Mystery\", \"3\": \"Music (Rock)\", \"4\": \"Comic Books\"})\n",
    "dummies.columns = [\"Drama / Film / Rock\", \"Juvinile Mystery\", \"United States / Biography\", \"Juvinile Literature / Biography\", \"Comic Books\"]\n",
    "list(dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame\n",
    "df_pandas_dummies = df_pandas.join(dummies, how='left')\n",
    "df_pandas_dummies\n",
    "list(df_pandas_dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = df_pandas.loc[(n+1):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_pandas_dummies, dummies, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "Library_Collection_Forecasting",
  "notebookId": 3995305047714984
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
