{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use case - predicting the demand for items at a libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfOptions = {\n",
    "#   \"sfURL\" : \"datalytyx.east-us-2.azure.snowflakecomputing.com\",\n",
    "#   \"sfAccount\" : \"datalytyx\",\n",
    "#   \"sfUser\" : \"WILLHOLTAM\",\n",
    "#   \"sfPassword\" : \"04MucSfLV\",\n",
    "#   \"sfRole\": \"DATABRICKS\",\n",
    "#   \"sfDatabase\" : \"DATABRICKS_DEMO\",\n",
    "#   \"sfSchema\" : \"SEATTLE_LIBRARY\",\n",
    "#   \"sfWarehouse\" : \"DATASCIENCE_WH\"\n",
    "# }\n",
    "# SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #spark.conf.set(\"spark.executor.cores\",2)\n",
    "\n",
    "# df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "#   .options(**sfOptions) \\\n",
    "#   .option(\"query\", \"\"\"select * from library_collection_inventory where reportdate in ('2017-09-01T00:00:00','2017-10-01T00:00:00', '2017-11-01T00:00:00', '2017-12-01T00:00:00', '2018-01-01T00:00:00', '2018-01-01T00:00:00', '2018-02-01T00:00:00', '2018-02-01T00:00:00', '2018-03-01T00:00:00', '2018-04-01T00:00:00', '2018-05-01T00:00:00', '2018-06-01T00:00:00', '2018-07-01T00:00:00') \"\"\") \\\n",
    "#   .load().limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a view or table\n",
    "\n",
    "# temp_table_name = \"library_collection_inventory\"\n",
    "\n",
    "# df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk  # Has to be added through Workspaces/ attach library to cluster when using Databricks\n",
    "import more_itertools\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import MDS\n",
    "import codecs\n",
    "import mpld3\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "from snowflake.sqlalchemy import URL\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = df.toPandas()  # Create pandas dataframe to work within python when in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set login parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_account = \"datalytyx\"\n",
    "sf_user = \"WILLHOLTAM\"\n",
    "sf_pwd = \"04MucSfLV\"\n",
    "# sf_user = \"CHRISSCHON\"\n",
    "# sf_pwd = \"UpsetSheep7\"\n",
    "sf_role = \"DATABRICKS\"\n",
    "sf_db = \"DATABRICKS_DEMO\"\n",
    "sf_schema = \"SEATTLE_LIBRARY\"\n",
    "sf_wh = \"DATASCIENCE_WH\"\n",
    "sf_region = \"east-us-2.azure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Snowflake login URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URL(\n",
    "    user = sf_user,\n",
    "    password = sf_pwd,\n",
    "    account = sf_account,\n",
    "    region = sf_region,\n",
    "    database = sf_db,\n",
    "    schema = sf_schema,\n",
    "    warehouse = sf_wh,\n",
    "    role = sf_role,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data from Snowflake as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_sql_query(\"select * from library_collection_inventory where reportdate in ('2017-09-01T00:00:00','2017-10-01T00:00:00', '2017-11-01T00:00:00', '2017-12-01T00:00:00', '2018-01-01T00:00:00', '2018-01-01T00:00:00', '2018-02-01T00:00:00', '2018-02-01T00:00:00', '2018-03-01T00:00:00', '2018-04-01T00:00:00', '2018-05-01T00:00:00', '2018-06-01T00:00:00', '2018-07-01T00:00:00') limit 1000\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bibnum</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>isbn</th>\n",
       "      <th>publicationyear</th>\n",
       "      <th>publisher</th>\n",
       "      <th>subjects</th>\n",
       "      <th>itemtype</th>\n",
       "      <th>itemcollection</th>\n",
       "      <th>floatingitem</th>\n",
       "      <th>itemlocation</th>\n",
       "      <th>reportdate</th>\n",
       "      <th>itemcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2793009</td>\n",
       "      <td>The perfect gentleman : a Muslim boy meets the...</td>\n",
       "      <td>Ahmad, Imran.</td>\n",
       "      <td>1455508497, 9781455508495</td>\n",
       "      <td>2012.</td>\n",
       "      <td>Center Street,</td>\n",
       "      <td>Ahmad Imran Childhood and youth, Muslim boys E...</td>\n",
       "      <td>acbk</td>\n",
       "      <td>nanf</td>\n",
       "      <td>NA</td>\n",
       "      <td>lcy</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2683003</td>\n",
       "      <td>Leo the snow leopard : the true story of an am...</td>\n",
       "      <td>Hatkoff, Juliana</td>\n",
       "      <td>0545229278, 9780545229272</td>\n",
       "      <td>2010.</td>\n",
       "      <td>Scholastic Press,</td>\n",
       "      <td>New York Zoological Park Juvenile literature, ...</td>\n",
       "      <td>jcbk</td>\n",
       "      <td>ncnf</td>\n",
       "      <td>NA</td>\n",
       "      <td>swt</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>751402</td>\n",
       "      <td>Designs for helicopters.</td>\n",
       "      <td>Laskowitz, Isidor B., 1896-</td>\n",
       "      <td>None</td>\n",
       "      <td>[1947]</td>\n",
       "      <td>[Eastern Printing Co.],</td>\n",
       "      <td>Helicopters Patents</td>\n",
       "      <td>arbk</td>\n",
       "      <td>caaero</td>\n",
       "      <td>NA</td>\n",
       "      <td>cen</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3078119</td>\n",
       "      <td>Kidô keisatsu patorebâ = Patlabor : the mobile...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[2015]</td>\n",
       "      <td>Maiden Japan,</td>\n",
       "      <td>Robotics Japan Drama, Police Japan Drama, Toky...</td>\n",
       "      <td>acdvd</td>\n",
       "      <td>nalndvd</td>\n",
       "      <td>Floating</td>\n",
       "      <td>dlr</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2747381</td>\n",
       "      <td>Southeast Asia's best recipes : from Bangkok t...</td>\n",
       "      <td>Hutton, Wendy</td>\n",
       "      <td>0804841667, 9780804841665</td>\n",
       "      <td>[2010]</td>\n",
       "      <td>Tuttle Pub.,</td>\n",
       "      <td>Cooking Southeast Asian, Cookbooks</td>\n",
       "      <td>acbk</td>\n",
       "      <td>nanf</td>\n",
       "      <td>NA</td>\n",
       "      <td>nhy</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bibnum                                              title  \\\n",
       "0  2793009  The perfect gentleman : a Muslim boy meets the...   \n",
       "1  2683003  Leo the snow leopard : the true story of an am...   \n",
       "2   751402                           Designs for helicopters.   \n",
       "3  3078119  Kidô keisatsu patorebâ = Patlabor : the mobile...   \n",
       "4  2747381  Southeast Asia's best recipes : from Bangkok t...   \n",
       "\n",
       "                        author                       isbn publicationyear  \\\n",
       "0                Ahmad, Imran.  1455508497, 9781455508495           2012.   \n",
       "1             Hatkoff, Juliana  0545229278, 9780545229272           2010.   \n",
       "2  Laskowitz, Isidor B., 1896-                       None          [1947]   \n",
       "3                         None                       None          [2015]   \n",
       "4                Hutton, Wendy  0804841667, 9780804841665          [2010]   \n",
       "\n",
       "                 publisher                                           subjects  \\\n",
       "0           Center Street,  Ahmad Imran Childhood and youth, Muslim boys E...   \n",
       "1        Scholastic Press,  New York Zoological Park Juvenile literature, ...   \n",
       "2  [Eastern Printing Co.],                                Helicopters Patents   \n",
       "3            Maiden Japan,  Robotics Japan Drama, Police Japan Drama, Toky...   \n",
       "4             Tuttle Pub.,                 Cooking Southeast Asian, Cookbooks   \n",
       "\n",
       "  itemtype itemcollection floatingitem itemlocation           reportdate  \\\n",
       "0     acbk           nanf           NA          lcy  2017-12-01T00:00:00   \n",
       "1     jcbk           ncnf           NA          swt  2017-12-01T00:00:00   \n",
       "2     arbk         caaero           NA          cen  2017-12-01T00:00:00   \n",
       "3    acdvd        nalndvd     Floating          dlr  2017-12-01T00:00:00   \n",
       "4     acbk           nanf           NA          nhy  2017-12-01T00:00:00   \n",
       "\n",
       "  itemcount  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download stopwords and sentence tokenizer from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WilliamHoltam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WilliamHoltam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Common words to ignore\n",
    "nltk.download('punkt')  # Punkt Sentence Tokenizer - more useful in large documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load nltk's English stopwords as variable called 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load nltk's SnowballStemmer as variabled 'stemmer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of clusters to be used later in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline processes the original dataframe so that the datatypes in the columns of interest are correct. It removes both empty columns and rows containing NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Be careful to return a dataframe containing the same indexes and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoneReplacer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer changes Nonetype values into numpy NaN values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Ttransform confirms X is a Dataframe and fills Nonetype with pd.np.nan\n",
    "        \"\"\"\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        X.fillna(value = pd.np.nan, inplace=True)\n",
    "        \n",
    "        return pd.DataFrame(X, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "initiated_class = NoneReplacer()\n",
    "\n",
    "initiated_class.fit(features)\n",
    "\n",
    "df_pandas_fitter = initiated_class.transform(features)\n",
    "# df_pandas_fitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes empty columns from dataframe.\n",
    "\n",
    "Note: Has to be in the fit as can't alter columns in Transform. Have to perform same columnwise opperations to both test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyColumnRemover(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer drops empty columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):  # has to take an optional y for pipelines\n",
    "        \n",
    "        \"\"\"\n",
    "        Fit Drops Columns where 100% of the column is Nonetype\n",
    "        \"\"\"\n",
    "        \n",
    "        self.drop_columns = self.df.isna().sum()[self.df.isna().sum() == X.shape[0]].index  # Calculates pd.series with column lables as indecies\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Transform Confirms X is a DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        return pd.DataFrame(X.drop(columns = self.drop_columns), index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "initiated_class = EmptyColumnRemover(df=features)\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter)\n",
    "\n",
    "df_pandas_fitter1 = initiated_class.transform(df_pandas_fitter)\n",
    "# df_pandas_fitter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A violent technique for dealing with NaNs - remove all rows containing NaNs, won't be appropriate for all usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyNaNRowRemover(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer removes any rows where here any element in row is NaN\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y = None):  # has to take an optional y for pipelines\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        Transform drops rows where any element in row is NaN \n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        X = X.dropna(axis=0, how='any')\n",
    "\n",
    "        self.cleaned_data = X\n",
    "\n",
    "        return pd.DataFrame(X, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "initiated_class = AnyNaNRowRemover()\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter1)\n",
    "\n",
    "df_pandas_fitter2 = initiated_class.transform(df_pandas_fitter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_pipeline = Pipeline([\n",
    "    ('nr', NoneReplacer()),\n",
    "    ('ecr', EmptyColumnRemover(df=features)),\n",
    "    ('anrr', AnyNaNRowRemover())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline performs the steps necessary to perform K-means clustering on natural language data stored as text including Tokenizing, Stemming, TF-IDF-Vectorizing and K-means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of TF-IDF-Vectorizing is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines two processes:\n",
    "    Tokenizing: Split sentences down into individual words.\n",
    "    Stemming: The process of breaking a word down into its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeAndStemer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer tokenizes and stems the words, creating a vocab dataframe \n",
    "    containing the words and their stems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_column):\n",
    "        self.text_column = text_column\n",
    "        pass\n",
    "    \n",
    "    def tokenize_and_stem(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function first Tokenizes and then Stems the words, returning the stems.\n",
    "        \"\"\"\n",
    "        \n",
    "        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "        \n",
    "        stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "        return stems\n",
    "    \n",
    "    def tokenize_only(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function Tokenizes the words, returning tokens that are filtered to contain upper or\n",
    "        lowwer case letters.\n",
    "        \"\"\" \n",
    "        \n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "        tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        \n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "                \n",
    "        return filtered_tokens\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Transform applies both of the custom functions defined above.\n",
    "        \n",
    "        The tokenize_and_stem function us used to create a flat list \n",
    "        containing the entirity of the vocab in stemmed form. \n",
    "        \n",
    "        The tokenize_only is used to create a flat list containing the\n",
    "        entirity of the vocab as a flat list.\n",
    "        \n",
    "        A dataframe is created from both of these flat lists with the\n",
    "        \"words\" column consisting of the individual words and the index\n",
    "        consisting of the stems.\n",
    "        \n",
    "        Returns the feature of interest as a list\n",
    "        \"\"\"\n",
    "\n",
    "        totalvocab_stemmed = []\n",
    "        \n",
    "        allwords_stemmed = [self.tokenize_and_stem(str(i)) for i in X[self.text_column].tolist()]  # for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed)\n",
    "        totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed))\n",
    "        \n",
    "        totalvocab_tokenized = []\n",
    "\n",
    "        allwords_tokenized = [self.tokenize_only(str(i)) for i in X[self.text_column].tolist()]  # for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized))        \n",
    " \n",
    "        self.vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "\n",
    "        print('there are ' + str(self.vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "        print(' ')\n",
    "        print(self.vocab_frame.head())\n",
    "        X = X[self.text_column].tolist()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9804 items in vocab_frame\n",
      " \n",
      "               words\n",
      "ahmad          ahmad\n",
      "imran          imran\n",
      "childhood  childhood\n",
      "and              and\n",
      "youth          youth\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "initiated_class = TokenizeAndStemer(text_column='subjects')\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter2)\n",
    "\n",
    "df_pandas_fitter3 = initiated_class.transform(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorCapture(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        self.tfidf_matrix = X\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansLabels(KMeans):\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        return super(KMeansLabels,self).transform(X).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Empty(TransformerMixin, BaseEstimator):\n",
    "    def transform(self, X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'max_df': 0.8,\n",
    "    'max_features': 20000,\n",
    "    'min_df': 0.001,\n",
    "    'stop_words': 'english',\n",
    "    'use_idf': True,\n",
    "    'tokenizer': TokenizeAndStemer(text_column='subjects').tokenize_and_stem,\n",
    "    'ngram_range': (1,3)\n",
    "}\n",
    "\n",
    "clustering_pipeline = Pipeline([\n",
    "    ('tas', TokenizeAndStemer(text_column='subjects')),\n",
    "    ('tfidf', TfidfVectorizer(**kwargs)),\n",
    "    ('vc', VectorCapture()),\n",
    "    ('km', KMeansLabels(\n",
    "        n_clusters=num_clusters,\n",
    "        random_state=1000)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline appends the clusters to the original dataframe so that further analysis can be done and the attributes of each cluster can be investigated.\n",
    "\n",
    "The top terms in each cluster are returned to the screen as an indicator of the topic/genre of each cluster.\n",
    "\n",
    "In Future:\n",
    "Want to Onehot encode the cluster categories and set the top terms as the column names for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataFrameFeatureUnion(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\" A DataFrame transformer that unites several DataFrame transformers\n",
    "    \n",
    "#     Fit several DataFrame transformers and provides a concatenated\n",
    "#     Data Frame\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     list_of_transformers : list of DataFrameTransformers\n",
    "        \n",
    "        \n",
    "#     Authors\n",
    "#     -------\n",
    "#     Guy who wrote this one: https://www.kaggle.com/jankoch/scikit-learn-pipelines-and-pandas\n",
    "#     \"\"\" \n",
    "    \n",
    "#     def __init__(self, list_of_transformers):\n",
    "\n",
    "#         self.list_of_transformers = list_of_transformers\n",
    "        \n",
    "\n",
    "#     def transform(self, X, **transformparamn):\n",
    "#         \"\"\" Applies the fitted transformers on a DataFrame\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : pandas DataFrame\n",
    "        \n",
    "#         Returns\n",
    "#         ----------\n",
    "#         concatted :  pandas DataFrame\n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "#         concatted = pd.concat([transformer.transform(X)\n",
    "#                             for transformer in\n",
    "#                             self.fitted_transformers_], axis=1).copy()\n",
    "#         return concatted\n",
    "\n",
    "\n",
    "#     def fit(self, X, y=None, **fitparams):\n",
    "#         \"\"\" Fits several DataFrame Transformers\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : pandas DataFrame\n",
    "#         y : not used, API requirement\n",
    "        \n",
    "#         Returns\n",
    "#         ----------\n",
    "#         self : object\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.fitted_transformers_ = []\n",
    "#         for transformer in self.list_of_transformers:\n",
    "#             fitted_trans = clone(transformer).fit(X, y=None, **fitparams)\n",
    "#             self.fitted_transformers_.append(fitted_trans)\n",
    "        \n",
    "#         return self\n",
    "    \n",
    "\n",
    "# #===========================================================================================    \n",
    "# #Custom Prebuilt Transformers\n",
    "# #==========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasFeatureUnion(FeatureUnion):\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \n",
    "        self._validate_transformers()\n",
    "        result = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_transform_one)(trans, weight, X, y,\n",
    "                                        **fit_params)\n",
    "            for name, trans, weight in self._iter())\n",
    "\n",
    "        if not result:\n",
    "            # All transformers are None\n",
    "            return np.zeros((X.shape[0], 0))\n",
    "        Xs, transformers = zip(*result)\n",
    "        self._update_transformer_list(transformers)\n",
    "        if any(sparse.issparse(f) for f in Xs):\n",
    "            Xs = sparse.hstack(Xs).tocsr()\n",
    "        else:\n",
    "            Xs = self.merge_dataframes_by_column(Xs)\n",
    "        return Xs\n",
    "\n",
    "    def merge_dataframes_by_column(self, Xs):\n",
    "        return pd.concat(Xs, axis=\"columns\", copy=False)\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xs = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_transform_one)(trans, weight, X)\n",
    "            for name, trans, weight in self._iter())\n",
    "        if not Xs:\n",
    "            # All transformers are None\n",
    "            return np.zeros((X.shape[0], 0))\n",
    "        if any(sparse.issparse(f) for f in Xs):\n",
    "            Xs = sparse.hstack(Xs).tocsr()\n",
    "        else:\n",
    "            Xs = self.merge_dataframes_by_column(Xs)\n",
    "        return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PDFeatureUnion(FeatureUnion):\n",
    "    \n",
    "#     def fit(argz):\n",
    "#         super(PDFeatureunion, self).fit(argz)\n",
    "#         return self\n",
    "    \n",
    "#     def transform(slef, X, argz):\n",
    "        \n",
    "#         nparray = super(PDFeatureUnion).transform(argz)\n",
    "        \n",
    "#         return pd.DataFrame(nparray, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameRebuild(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    \"\"\"\n",
    "    Transformer concatinates the cleaned dataframe after the pre-processing and\n",
    "    the cluster categores as a new column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Transform concatinates the cleaned dataframe after the pre-processing and\n",
    "        the cluster categores as a new column and returns the new \n",
    "        \"\"\"\n",
    "        \n",
    "        self.clusters = clustering_pipeline.named_steps['km'].labels_.tolist()\n",
    "        clusters_df = pd.DataFrame(self.clusters, columns = ['clusters'])\n",
    "        clusters_df = clusters_df.set_index(pre_processing_pipeline.get_params(True)['anrr'].cleaned_data.index)\n",
    "        \n",
    "        cleaned_data = pre_processing_pipeline.get_params(True)['anrr'].cleaned_data\n",
    "        \n",
    "        result = pd.concat([cleaned_data, clusters_df], axis=1)\n",
    "        \n",
    "        result = result.set_index(\n",
    "            keys = 'clusters',\n",
    "            drop=False,\n",
    "            append=False,\n",
    "            inplace=False,\n",
    "            verify_integrity=False\n",
    "        )\n",
    "        \n",
    "        # frame.columns\n",
    "        print(result.clusters.value_counts()) #number of items per cluster (clusters from 0 to 4)\n",
    "        \n",
    "        return result #pd.DataFrame(result, index = .index, columns = X.columns)  # df_clustering_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rom __future__ import print_function  # Print() becomes a function\n",
    "\n",
    "class TopTerms(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer prints out words closest to centre of each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Transform returns the terms the tfidf matrix represents.\n",
    "        Orders the cluster centers by proximity to centroid.\n",
    "        Prints terms by proximity to centroid.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Top terms per cluster:\")\n",
    "        print()\n",
    "        \n",
    "        terms = clustering_pipeline.named_steps['tfidf'].get_feature_names()  # the terms tfidf matrix represents\n",
    "        \n",
    "        # sort cluster centers by proximity to centroid\n",
    "        order_centroids = pd.DataFrame(\n",
    "            clustering_pipeline.\n",
    "            named_steps['km'].\n",
    "            cluster_centers_.\n",
    "            argsort()[:, ::-1]\n",
    "        )\n",
    "        \n",
    "        self.top_terms_list = []\n",
    "        \n",
    "        for i in range(num_clusters):\n",
    "            print(\"cluster %d words:\" % i, end='')\n",
    "            \n",
    "            clustering_pipeline.get_params(True)['tas'].vocab_frame\n",
    "            \n",
    "            top_terms = []\n",
    "            for ind in order_centroids.iloc[i, :6]:\n",
    "                print(' %s' % clustering_pipeline.\n",
    "                      get_params(True)['tas'].\n",
    "                      vocab_frame.\n",
    "                      loc[terms[ind].split(' ')].\n",
    "                      values.tolist()[0][0].\n",
    "                      encode('utf-8', 'ignore'),\n",
    "                      end=','\n",
    "                     )\n",
    "                \n",
    "                top_terms.append((\n",
    "                    ' %s' % clustering_pipeline.\n",
    "                    get_params(True)['tas'].\n",
    "                    vocab_frame.\n",
    "                    loc[terms[ind].split(' ')].\n",
    "                    values.tolist()[0][0] + ','#.\n",
    "#                     encode('utf-8', 'ignore')\n",
    "                ).strip())\n",
    "            \n",
    "                print() #add whitespace\n",
    "                print() #add whitespace\n",
    "                \n",
    "#                 print(\"Cluster %d titles:\" % i, end='')\n",
    "# #                 for title in X.loc[i]['title'].values.tolist():\n",
    "# #                     print(' %s,' % title, end='')\n",
    "#                 print() #add whitespace\n",
    "#                 print() #add whitespace\n",
    "#                 top_terms = str[top_terms[:]]\n",
    "            self.top_terms_list.append(\"\".join(top_terms))\n",
    "    \n",
    "            print()\n",
    "            print()\n",
    "    \n",
    "        return pd.DataFrame(X, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs = {\n",
    "#     'original_data': pre_processing_pipeline.get_params(True)['anrr'].cleaned_data,\n",
    "#     'new_data': clustering_pipeline.named_steps['km'],\n",
    "#     'new_column_names': 'clusters'\n",
    "# }\n",
    "\n",
    "post_processing_pipeline = Pipeline([\n",
    "    ('dfr', DataFrameRebuild()),\n",
    "    ('tt', TopTerms())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_fit_transform_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-6afd3473041e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mresult_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-8dbb775666ff>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m      6\u001b[0m             delayed(_fit_transform_one)(trans, weight, X, y,\n\u001b[0;32m      7\u001b[0m                                         **fit_params)\n\u001b[1;32m----> 8\u001b[1;33m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    818\u001b[0m             tasks = BatchedCalls(itertools.islice(iterator, batch_size),\n\u001b[0;32m    819\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nested_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m                                  self._pickle_cache)\n\u001b[0m\u001b[0;32m    821\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m                 \u001b[1;31m# No more tasks available in the iterator: tell caller to stop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterator_slice, backend, pickle_cache)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_slice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-8dbb775666ff>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m             delayed(_fit_transform_one)(trans, weight, X, y,\n\u001b[0;32m      7\u001b[0m                                         **fit_params)\n\u001b[1;32m----> 8\u001b[1;33m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_fit_transform_one' is not defined"
     ]
    }
   ],
   "source": [
    "model_pipeline = Pipeline([\n",
    "    ('pre_p_pipe', pre_processing_pipeline),\n",
    "    ('feats', PandasFeatureUnion([\n",
    "        ('empty', Empty),\n",
    "        ('c_pipe', clustering_pipeline)\n",
    "    ])),\n",
    "#      ('post_p_pipe', post_processing_pipeline)\n",
    "])\n",
    "\n",
    "result_df = model_pipeline.fit_transform(X = features)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pipeline = Pipeline([\n",
    "#     ('pre_p_pipe', pre_processing_pipeline),\n",
    "#     ('c_pipe', clustering_pipeline),\n",
    "#     ('post_p_pipe', post_processing_pipeline)\n",
    "# ])\n",
    "\n",
    "# result_df = model_pipeline.fit_transform(X = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceCosineSimilarity(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, tfidf_matrix):\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        dist = 1 - cosine_similarity(self.tfidf_matrix)  # clustering_pipeline.named_steps['vc'].tfidf_matrix  \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "initiated_class = DistanceCosineSimilarity(tfidf_matrix = clustering_pipeline.named_steps['vc'].tfidf_matrix)\n",
    "\n",
    "initiated_class.fit(model_pipeline)\n",
    "\n",
    "df_pandas_fitter1 = initiated_class.transform(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiObservationKMeansVisualisation(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "        pos = mds.fit_transform(X)\n",
    "        \n",
    "        xs, ys = pos[:, 0], pos[:, 1]\n",
    "        \n",
    "        cluster_numbers = list(range(num_clusters))\n",
    "        cluster_description = post_processing_pipeline.named_steps['tt'].top_terms_list\n",
    "\n",
    "        cluster_names = dict(zip(cluster_numbers, cluster_description))\n",
    "        \n",
    "        %matplotlib inline\n",
    "        \n",
    "        clusters = post_processing_pipeline.named_steps['dfanc'].clusters\n",
    "        titles = pre_processing_pipeline.get_params(True)['anrr'].cleaned_data.title\n",
    "\n",
    "        df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles))\n",
    "        \n",
    "        #group by cluster\n",
    "        groups = df.groupby('label')\n",
    "        \n",
    "        # set up plot\n",
    "        fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "        ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "        \n",
    "        # iterate through groups to layer the plot\n",
    "        # note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "        for name, group in groups:\n",
    "            ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "                    label=cluster_names[name],\n",
    "                    mec='none')\n",
    "            ax.set_aspect('auto')\n",
    "            ax.tick_params(\\\n",
    "                axis= 'x',          # changes apply to the x-axis\n",
    "                which='both',      # both major and minor ticks are affected\n",
    "                bottom=False,      # ticks along the bottom edge are off\n",
    "                top=False,         # ticks along the top edge are off\n",
    "                labelbottom=False)\n",
    "            ax.tick_params(\\\n",
    "                axis= 'y',         # changes apply to the y-axis\n",
    "                which='both',      # both major and minor ticks are affected\n",
    "                left=False,      # ticks along the bottom edge are off\n",
    "                top=False,         # ticks along the top edge are off\n",
    "                labelleft=False)\n",
    "            \n",
    "        ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "            \n",
    "#         plt.show() #show the plot\n",
    "        \n",
    "        return plt.show()\n",
    "        \n",
    "        # #uncomment the below to save the plot if need be\n",
    "        # #plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing\n",
    "\n",
    "# initiated_class = MultiObservationKMeansVisualisation()\n",
    "\n",
    "# initiated_class.fit(pos)\n",
    "\n",
    "# df_pandas_fitter2 = initiated_class.transform(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_pipeline = Pipeline([\n",
    "    ('dcs', DistanceCosineSimilarity(tfidf_matrix=clustering_pipeline.named_steps['vc'].tfidf_matrix)),\n",
    "    ('mokmv', MultiObservationKMeansVisualisation())\n",
    "])\n",
    "\n",
    "visualisation_pipeline.fit_transform(X = model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dist = 1 - cosine_similarity(clustering_pipeline.named_steps['vc'].tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "# cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', }\n",
    "\n",
    "cluster_numbers = list(range(num_clusters))\n",
    "cluster_description = post_processing_pipeline.named_steps['tt'].top_terms_list\n",
    "\n",
    "cluster_names = dict(zip(cluster_numbers, cluster_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some ipython magic to show the matplotlib plots inline\n",
    "%matplotlib inline \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "\n",
    "clusters = post_processing_pipeline.named_steps['dfr'].clusters\n",
    "titles = pre_processing_pipeline.get_params(True)['anrr'].cleaned_data.title\n",
    "\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# iterate through groups to layer the plot\n",
    "# note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "#     print(name)\n",
    "#    print(group)\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name],\n",
    "#             color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "# #add label in x,y position with the label as the film title\n",
    "# for i in range(len(df)):\n",
    "#     ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "# #uncomment the below to save the plot if need be\n",
    "# #plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenizeAndStemer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "#     def __init__(self, default_column = 'subjects'):\n",
    "#         self.default_column = default_column\n",
    "#         pass\n",
    "    \n",
    "#     def tokenize_and_stem(self, X):\n",
    "        \n",
    "#         tokens = [word for sent in nltk.sent_tokenize(X) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "#         filtered_tokens = []\n",
    "#         [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "        \n",
    "#         stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "#         return stems\n",
    "    \n",
    "#     def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "\n",
    "#         totalvocab_stemmed = []\n",
    "        \n",
    "#         allwords_stemmed = [self.tokenize_and_stem(str(i)) for i in X[self.default_column].tolist()] #for each item in 'synopses', tokenize/stem\n",
    "#         totalvocab_stemmed.extend(allwords_stemmed)\n",
    "#         totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed))\n",
    "        \n",
    "#         return totalvocab_stemmed\n",
    "\n",
    "# # How to implement in class above???? OR do I have to implement it in a separate class...  \n",
    "# # allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# # totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# # totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenizeOnly(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "#     def __init__(self, default_column = 'subjects'):\n",
    "#         self.default_column = default_column\n",
    "#         pass\n",
    "\n",
    "#     def tokenize_only(self, X):\n",
    "        \n",
    "#         # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#         tokens = [word.lower() for sent in nltk.sent_tokenize(X) for word in nltk.word_tokenize(sent)]\n",
    "#         filtered_tokens = []\n",
    "        \n",
    "#         # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#         [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "                \n",
    "#         return filtered_tokens\n",
    "    \n",
    "#     def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "\n",
    "#         totalvocab_tokenized = []\n",
    "    \n",
    "#         allwords_tokenized = [self.tokenize_only(str(i)) for i in X[self.default_column].tolist()] #for each item in 'synopses', tokenize/stem\n",
    "#         totalvocab_tokenized.extend(allwords_tokenized)\n",
    "#         totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized))\n",
    "        \n",
    "#         return totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiated_class = TokenizeOnly()\n",
    "\n",
    "# initiated_class.fit(df_pandas_fitter2)\n",
    "\n",
    "# df_pandas_fitter3 = initiated_class.transform(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union = FeatureUnion([(\"tas\", TokenizeAndStemer()),\n",
    "#                       (\"to\", TokenizeOnly())])\n",
    "\n",
    "# test = union.fit_transform(df_pandas_fitter2)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenizeOnly(TransformerMixin, BaseEstimator):\n",
    "\n",
    "#     def __init__(self, default_column = 'subjects'):\n",
    "#         self.default_column = default_column\n",
    "#         pass\n",
    "    \n",
    "#      def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "# print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pre_processing_pipeline.fit_transform(X = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_stem_pipeline = Pipeline([\n",
    "    ('tas', TokenizeAndStemer())\n",
    "])\n",
    "\n",
    "list_tokenized_and_stemmed = tokenize_and_stem_pipeline.fit_transform(X = df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_only_pipeline = Pipeline([   \n",
    "    ('to', TokenizeOnly())\n",
    "])\n",
    "\n",
    "list_tokenized = tokenize_only_pipeline.fit_transform(X = df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(X, default_column = 'subjects'):\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(X[default_column]) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "#     filtered_tokens = []\n",
    "#     [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "    \n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "#     return stems\n",
    "\n",
    "# tokenize_and_stem(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "# tokenize_and_stem(df_pandas_fitter2['subjects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = df_pandas['subjects']\n",
    "\n",
    "# Define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "# #class TextTransform()\n",
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "\n",
    "# def tokenize_only(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     return filtered_tokens\n",
    "\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "\n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) \n",
    "\n",
    "# allwords_tokenized = [tokenize_only(str(i)) for i in subjects]\n",
    "# totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in subjects_c:\n",
    "#     allwords_stemmed = tokenize_and_stem(str(i)) #for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(str(i))\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return pd.DataFrame(self.model.predict(X))\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('cluster', ModelTransformer(KMeans_foo(3))),\n",
    "#     ('binarize', LabelBinarizer())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.loc[:,'PUBLICATIONYEAR'] = df_pandas.loc[:,'PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n",
    "# _______________________________________________________________________________\n",
    "# Functions for transforming the data\n",
    "# def processing(df):\n",
    "#     df['PUBLICATIONYEAR'] = df['PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "#     [str(item) for item in df['SUBJECTS'] if item is None]\n",
    "\n",
    "# processing(df_pandas)\n",
    "    \n",
    "    # def ext_date_fun(input, output):\n",
    "#   output = input.str.extractstr.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "  \n",
    "# def fix_none_fun(sub):\n",
    "#   [str(item) for item in sub if item is None]\n",
    "# _______________________________________________________________________________\n",
    "\n",
    "# # Attempt at putting the functions into classes\n",
    "# class PublicationYearCleaner(object):\n",
    "#   \"\"\"Preprocessing: This class cleans the Publication Year Column\"\"\"\n",
    "#   def __init__(self, data):\n",
    "#     self.raw = data\n",
    "  \n",
    "#   def ext_date(self, pub_year):\n",
    "#      pub_year = pub_year.str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n",
    "# class SubjectsCleaner(objct)\n",
    "#   def fix_none(self, string):\n",
    "#     if string is None:\n",
    "#       return ''\n",
    "#     return str(string)\n",
    "# # ________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.loc[:,'PUBLICATIONYEAR'] = df_pandas.loc[:,'PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000  # Number of rows to analyse\n",
    "\n",
    "# titles = df_pandas.loc[0:n,'TITLE'].values.tolist()\n",
    "# subjects = df_pandas.loc[0:n,'SUBJECTS'].values.tolist()\n",
    "# author = df_pandas.loc[0:n, 'AUTHOR'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return pd.DataFrame(self.model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMTYPE']), how='inner')\n",
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMCOLLECTION']), how='inner')\n",
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMLOCATION']), how='inner')\n",
    "# list(df_pandas.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use extend so it's a big flat list of vocab\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in df_pandas['subjects_c']:\n",
    "#     allwords_stemmed = tokenize_and_stem(str(i)) #for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(str(i))\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "vectorizer_pipe = Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=5, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3)))])\n",
    "\n",
    "%time tfidf_matrix = vectorizer_pipe.fit_transform(subjects)  # fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "dist = 1 - euclidean_distances(tfidf_matrix)\n",
    "\n",
    "cluster_pipe = Pipeline(steps=[('cluster', KMeans(n_clusters = 5, random_state=3425))])\n",
    "%time cluster_pipe.fit(tfidf_matrix)\n",
    "clusters = cluster_pipe.named_steps['cluster'].labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(cluster_pipe,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = cluster_pipe.named_steps['cluster'].labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = { 'title': titles, 'author': author, 'subjects': subjects_c, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(books, index = [clusters] , columns = ['title', 'author', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame.columns\n",
    "frame.cluster.value_counts() #number of books per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(ind)\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = cluster_pipe.named_steps['cluster'].cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace\n",
    "    \n",
    "    for ind in order_centroids[i, :3]: #replace 6 with n words per cluster\n",
    "        x += ' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=','\n",
    "    \n",
    "#    print(\"Cluster %d titles:\" % i, end='')\n",
    "#    for title in frame.loc[i,'title'].values.tolist():\n",
    "#        print(' %s,' % title, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os  # for os.path.basename\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# from sklearn.manifold import MDS\n",
    "\n",
    "# MDS()\n",
    "\n",
    "# # convert two components as we're plotting points in a two-dimensional plane\n",
    "# # \"precomputed\" because we provide a distance matrix\n",
    "# # we will also specify `random_state` so the plot is reproducible.\n",
    "# mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "# pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "# xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(frame.cluster)\n",
    "dummies.columns = dummies.columns.astype(str)\n",
    "list(dummies.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies.rename(index=str, columns={\"0\": \"Juvenile Literature\", \"1\": \"Music (Country)\", \"2\": \"Mystery\", \"3\": \"Music (Rock)\", \"4\": \"Comic Books\"})\n",
    "dummies.columns = [\"Drama / Film / Rock\", \"Juvinile Mystery\", \"United States / Biography\", \"Juvinile Literature / Biography\", \"Comic Books\"]\n",
    "list(dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame\n",
    "df_pandas_dummies = df_pandas.join(dummies, how='left')\n",
    "df_pandas_dummies\n",
    "list(df_pandas_dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = df_pandas.loc[(n+1):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_pandas_dummies, dummies, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "Library_Collection_Forecasting",
  "notebookId": 3995305047714984
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
