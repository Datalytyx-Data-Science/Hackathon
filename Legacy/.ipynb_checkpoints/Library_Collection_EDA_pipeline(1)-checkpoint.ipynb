{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case - predicting the demand for items at a libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfOptions = {\n",
    "#   \"sfURL\" : \"datalytyx.east-us-2.azure.snowflakecomputing.com\",\n",
    "#   \"sfAccount\" : \"datalytyx\",\n",
    "#   \"sfUser\" : \"WILLHOLTAM\",\n",
    "#   \"sfPassword\" : \"04MucSfLV\",\n",
    "#   \"sfRole\": \"DATABRICKS\",\n",
    "#   \"sfDatabase\" : \"DATABRICKS_DEMO\",\n",
    "#   \"sfSchema\" : \"SEATTLE_LIBRARY\",\n",
    "#   \"sfWarehouse\" : \"DATASCIENCE_WH\"\n",
    "# }\n",
    "# SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #spark.conf.set(\"spark.executor.cores\",2)\n",
    "\n",
    "# df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "#   .options(**sfOptions) \\\n",
    "#   .option(\"query\", \"\"\"select * from library_collection_inventory where reportdate in ('2017-09-01T00:00:00','2017-10-01T00:00:00', '2017-11-01T00:00:00', '2017-12-01T00:00:00', '2018-01-01T00:00:00', '2018-01-01T00:00:00', '2018-02-01T00:00:00', '2018-02-01T00:00:00', '2018-03-01T00:00:00', '2018-04-01T00:00:00', '2018-05-01T00:00:00', '2018-06-01T00:00:00', '2018-07-01T00:00:00') \"\"\") \\\n",
    "#   .load().limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a view or table\n",
    "\n",
    "# temp_table_name = \"library_collection_inventory\"\n",
    "\n",
    "# df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk  # Has to be added through Workspaces/ attach library to cluster\n",
    "import more_itertools\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import mpld3\n",
    "from snowflake.sqlalchemy import URL\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = df.toPandas()  # Create pandas dataframe to work within python when in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_account = \"datalytyx\"\n",
    "sf_user = \"WILLHOLTAM\"\n",
    "sf_pwd = \"04MucSfLV\"\n",
    "# sf_user = \"CHRISSCHON\"\n",
    "# sf_pwd = \"UpsetSheep7\"\n",
    "sf_role = \"DATABRICKS\"\n",
    "sf_db = \"DATABRICKS_DEMO\"\n",
    "sf_schema = \"SEATTLE_LIBRARY\"\n",
    "sf_wh = \"DATASCIENCE_WH\"\n",
    "sf_region = \"east-us-2.azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URL(\n",
    "    user = sf_user,\n",
    "    password = sf_pwd,\n",
    "    account = sf_account,\n",
    "    region = sf_region,\n",
    "    database = sf_db,\n",
    "    schema = sf_schema,\n",
    "    warehouse = sf_wh,\n",
    "    role = sf_role,\n",
    "))\n",
    "\n",
    "# engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_sql_query(\"select * from library_collection_inventory where reportdate in ('2017-09-01T00:00:00','2017-10-01T00:00:00', '2017-11-01T00:00:00', '2017-12-01T00:00:00', '2018-01-01T00:00:00', '2018-01-01T00:00:00', '2018-02-01T00:00:00', '2018-02-01T00:00:00', '2018-03-01T00:00:00', '2018-04-01T00:00:00', '2018-05-01T00:00:00', '2018-06-01T00:00:00', '2018-07-01T00:00:00') limit 1000\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WilliamHoltam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WilliamHoltam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Common words to ignore\n",
    "nltk.download('punkt')  # Punkt Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.dropna(axis=1, how='all')  # Drop the columns where all of the elements are missing values\n",
    "# df_pandas.dropna(axis=0, how='any')  # Drop the rows where any of the elements are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoneReplacer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer changes Nonetype values into numpy NaN values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "#        X_fitted = X.where(X == None)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        #[\"\" if item is None else str(item) for item in X.select_dtypes(include='object')]\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        X.fillna(value = pd.np.nan, inplace=True)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = NoneReplacer()\n",
    "\n",
    "initiated_class.fit(features)\n",
    "\n",
    "df_pandas_fitter = initiated_class.transform(features)\n",
    "# df_pandas_fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyColumnRemover(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer drops empty columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):  # has to take an optional y for pipelines\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the number of missing values that corresponds to the threshold.\n",
    "        Detects and labels columns with equal to or greater than numbers of missing values than the threshold.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.drop_columns = features.isna().sum()[features.isna().sum() >= X.shape[0]].index  # Calculates pd.series with column lables as indecies\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Drops columns containing empty values.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        return X.drop(columns = self.drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = EmptyColumnRemover()\n",
    "\n",
    "initiated_class.fit(X=df_pandas_fitter)\n",
    "\n",
    "df_pandas_fitter1 = initiated_class.transform(df_pandas_fitter)\n",
    "# df_pandas_fitter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyNaNRowRemover(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y = None):  # has to take an optional y for pipelines\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the number of missing values the corresponds to the threshold.\n",
    "        Detects and labels columns with more missing values that the threshold. \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        return X.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = AnyNaNRowRemover()\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter1)\n",
    "\n",
    "df_pandas_fitter2 = initiated_class.transform(df_pandas_fitter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeAndStemer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, default_column = 'subjects'):\n",
    "        self.default_column = default_column\n",
    "        pass\n",
    "    \n",
    "    def tokenize_and_stem(self, X):\n",
    "        \n",
    "        tokens = [word for sent in nltk.sent_tokenize(X) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "        \n",
    "        stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "        return stems\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        totalvocab_stemmed = []\n",
    "    \n",
    "        allwords_stemmed = [self.tokenize_and_stem(str(i)) for i in X[self.default_column].tolist()] #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed)\n",
    "        totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed))\n",
    "        \n",
    "        return totalvocab_stemmed\n",
    "\n",
    "# How to implement in class above???? OR do I have to implement it in a separate class...  \n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiated_class = TokenizeAndStemer()\n",
    "\n",
    "initiated_class.fit(df_pandas_fitter2)\n",
    "\n",
    "df_pandas_fitter3 = initiated_class.transform(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeOnly(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, default_column = 'subjects'):\n",
    "        self.default_column = default_column\n",
    "        pass\n",
    "\n",
    "    def tokenize_only(text):\n",
    "        \n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "        tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        \n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "                \n",
    "        return filtered_tokens\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        totalvocab_tokenized = []\n",
    "    \n",
    "        allwords_tokenized = [self.tokenize_only(str(i)) for i in X[self.default_column].tolist()] #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized))\n",
    "        \n",
    "        return totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = Pipeline([\n",
    "    ('nr', NoneReplacer()),\n",
    "    ('ecr', EmptyColumnRemover()),\n",
    "    ('anrr', AnyNaNRowRemover()),\n",
    "    ('tas', TokenizeAndStemer()),\n",
    "#     ('to', TokenizeOnly())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['financ',\n",
       " 'figur',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'dictionari',\n",
       " 'dinosaur',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'kiss',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'humor',\n",
       " 'stori',\n",
       " 'privat',\n",
       " 'investig',\n",
       " 'england',\n",
       " 'london',\n",
       " 'fiction',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'relationship',\n",
       " 'fiction',\n",
       " 'larg',\n",
       " 'type',\n",
       " 'book',\n",
       " 'mysteri',\n",
       " 'fiction',\n",
       " 'suspens',\n",
       " 'fiction',\n",
       " 'infant',\n",
       " 'nutrit',\n",
       " 'food',\n",
       " 'habit',\n",
       " 'habit',\n",
       " 'habit',\n",
       " 'break',\n",
       " 'chang',\n",
       " 'psycholog',\n",
       " 'behavior',\n",
       " 'modif',\n",
       " 'state',\n",
       " 'sponsor',\n",
       " 'terror',\n",
       " 'fiction',\n",
       " 'totalitarian',\n",
       " 'fiction',\n",
       " 'book',\n",
       " 'burn',\n",
       " 'fiction',\n",
       " 'censorship',\n",
       " 'fiction',\n",
       " 'polit',\n",
       " 'fiction',\n",
       " 'satir',\n",
       " 'literatur',\n",
       " 'scienc',\n",
       " 'fiction',\n",
       " 'cook',\n",
       " 'french',\n",
       " 'tom',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'hanna',\n",
       " 'and',\n",
       " 'barbera',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'jerri',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'hanna',\n",
       " 'and',\n",
       " 'barbera',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'cat',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'mice',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'humor',\n",
       " 'stori',\n",
       " 'plantat',\n",
       " 'life',\n",
       " 'north',\n",
       " 'carolina',\n",
       " 'fiction',\n",
       " 'larg',\n",
       " 'type',\n",
       " 'book',\n",
       " 'north',\n",
       " 'carolina',\n",
       " 'fiction',\n",
       " 'romanc',\n",
       " 'fiction',\n",
       " 'water',\n",
       " 'suppli',\n",
       " 'colorado',\n",
       " 'histori',\n",
       " 'colorado',\n",
       " 'environment',\n",
       " 'condit',\n",
       " 'histori',\n",
       " 'great',\n",
       " 'britain',\n",
       " 'armi',\n",
       " 'histori',\n",
       " 'world',\n",
       " 'war',\n",
       " 'franc',\n",
       " 'arm‚',\n",
       " 'histori',\n",
       " 'world',\n",
       " 'war',\n",
       " 'germani',\n",
       " 'heer',\n",
       " 'histori',\n",
       " 'world',\n",
       " 'war',\n",
       " 'somm',\n",
       " '1st',\n",
       " 'battl',\n",
       " 'of',\n",
       " 'the',\n",
       " 'franc',\n",
       " 'draw',\n",
       " 'techniqu',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'love',\n",
       " 'intimaci',\n",
       " 'psycholog',\n",
       " 'conduct',\n",
       " 'of',\n",
       " 'life',\n",
       " 'compass',\n",
       " 'superhero',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'friendship',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'butterfli',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'shackleton',\n",
       " 'ernest',\n",
       " 'henri',\n",
       " 'sir',\n",
       " 'explor',\n",
       " 'great',\n",
       " 'britain',\n",
       " 'biographi',\n",
       " 'antarctica',\n",
       " 'discoveri',\n",
       " 'and',\n",
       " 'explor',\n",
       " 'dinosaur',\n",
       " 'in',\n",
       " 'art',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'draw',\n",
       " 'techniqu',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'figur',\n",
       " 'draw',\n",
       " 'techniqu',\n",
       " 'femal',\n",
       " 'nude',\n",
       " 'in',\n",
       " 'art',\n",
       " 'human',\n",
       " 'figur',\n",
       " 'in',\n",
       " 'art',\n",
       " 'illustr',\n",
       " 'book',\n",
       " 'albania',\n",
       " 'histori',\n",
       " 'axi',\n",
       " 'occup',\n",
       " 'fiction',\n",
       " 'albania',\n",
       " 'histori',\n",
       " 'fiction',\n",
       " 'histor',\n",
       " 'fiction',\n",
       " 'corot',\n",
       " 'jean',\n",
       " 'baptist',\n",
       " 'camill',\n",
       " 'folklor',\n",
       " 'india',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'peacock',\n",
       " 'folklor',\n",
       " 'dragon',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'flight',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'mother',\n",
       " 'and',\n",
       " 'child',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'butterfli',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'butterfli',\n",
       " 'ecolog',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'butterfli',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'human',\n",
       " 'be',\n",
       " 'on',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'smith',\n",
       " 'kiki',\n",
       " 'exhibit',\n",
       " 'dinosaur',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'paleontolog',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'fossil',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'dinosaur',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'paleontolog',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'fossil',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'cartoon',\n",
       " 'and',\n",
       " 'comic',\n",
       " 'nonfict',\n",
       " 'comic',\n",
       " 'educ',\n",
       " 'comic',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'toddler',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'japan',\n",
       " 'translat',\n",
       " 'into',\n",
       " 'english',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'japan',\n",
       " 'translat',\n",
       " 'into',\n",
       " 'english',\n",
       " 'byrn',\n",
       " 'gari',\n",
       " 'j',\n",
       " 'clinton',\n",
       " 'bill',\n",
       " 'clinton',\n",
       " 'hillari',\n",
       " 'rodham',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'secret',\n",
       " 'servic',\n",
       " 'offici',\n",
       " 'and',\n",
       " 'employe',\n",
       " 'biographi',\n",
       " 'polit',\n",
       " 'corrupt',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'presid',\n",
       " 'protect',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'presid',\n",
       " 'spous',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'lauren',\n",
       " 'jillian',\n",
       " 'prostitut',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'biographi',\n",
       " 'harem',\n",
       " 'borneo',\n",
       " 'ident',\n",
       " 'psycholog',\n",
       " 'sagan',\n",
       " 'carl',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'astronom',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'biographi',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'astronom',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'wimsey',\n",
       " 'peter',\n",
       " 'lord',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'fiction',\n",
       " 'privat',\n",
       " 'investig',\n",
       " 'england',\n",
       " 'fiction',\n",
       " 'murder',\n",
       " 'investig',\n",
       " 'fiction',\n",
       " 'artist',\n",
       " 'scotland',\n",
       " 'fiction',\n",
       " 'kirkcudbright',\n",
       " 'scotland',\n",
       " 'fiction',\n",
       " 'mysteri',\n",
       " 'fiction',\n",
       " 'time',\n",
       " 'travel',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'frontier',\n",
       " 'and',\n",
       " 'pioneer',\n",
       " 'life',\n",
       " 'kansa',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'tornado',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'magic',\n",
       " 'fiction',\n",
       " 'tree',\n",
       " 'hous',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'kansa',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'war',\n",
       " 'crimin',\n",
       " 'fiction',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'relationship',\n",
       " 'fiction',\n",
       " 'ireland',\n",
       " 'fiction',\n",
       " 'clam',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'automobil',\n",
       " 'repair',\n",
       " 'shop',\n",
       " 'fiction',\n",
       " 'motherless',\n",
       " 'famili',\n",
       " 'fiction',\n",
       " 'depress',\n",
       " 'fiction',\n",
       " 'rabi',\n",
       " 'fiction',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'highway',\n",
       " 'fiction',\n",
       " 'oklahoma',\n",
       " 'fiction',\n",
       " 'histor',\n",
       " 'fiction',\n",
       " 'romant',\n",
       " 'suspens',\n",
       " 'fiction',\n",
       " 'witch',\n",
       " 'fiction',\n",
       " 'witchcraft',\n",
       " 'fiction',\n",
       " 'spanish',\n",
       " 'languag',\n",
       " 'materi',\n",
       " 'teenag',\n",
       " 'girl',\n",
       " 'fiction',\n",
       " 'devil',\n",
       " 'fiction',\n",
       " 'magic',\n",
       " 'fiction',\n",
       " 'paranorm',\n",
       " 'fiction',\n",
       " 'scienc',\n",
       " 'fiction',\n",
       " 'western',\n",
       " 'fiction',\n",
       " 'fantasi',\n",
       " 'fiction',\n",
       " 'danc',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'school',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'stage',\n",
       " 'fright',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'friendship',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'zombi',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'stori',\n",
       " 'in',\n",
       " 'rhyme',\n",
       " 'stori',\n",
       " 'in',\n",
       " 'rhyme',\n",
       " 'space',\n",
       " 'and',\n",
       " 'time',\n",
       " 'fiction',\n",
       " 'magic',\n",
       " 'fiction',\n",
       " 'fantasi',\n",
       " 'indian',\n",
       " 'philosophi',\n",
       " 'fiction',\n",
       " 'sculptor',\n",
       " 'fiction',\n",
       " 'fantasi',\n",
       " 'magician',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'good',\n",
       " 'and',\n",
       " 'evil',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'kangaroo',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'african',\n",
       " 'american',\n",
       " 'teenag',\n",
       " 'girl',\n",
       " 'fiction',\n",
       " 'femal',\n",
       " 'friendship',\n",
       " 'fiction',\n",
       " 'kidnap',\n",
       " 'fiction',\n",
       " 'reveng',\n",
       " 'fiction',\n",
       " 'inner',\n",
       " 'citi',\n",
       " 'fiction',\n",
       " 'urban',\n",
       " 'fiction',\n",
       " 'london',\n",
       " 'england',\n",
       " 'histori',\n",
       " 'fiction',\n",
       " 'fantasi',\n",
       " 'fiction',\n",
       " 'mayan',\n",
       " 'languag',\n",
       " 'write',\n",
       " 'archaeolog',\n",
       " 'mexico',\n",
       " 'histori',\n",
       " 'archaeolog',\n",
       " 'central',\n",
       " 'america',\n",
       " 'histori',\n",
       " 'mexico',\n",
       " 'antiqu',\n",
       " 'central',\n",
       " 'america',\n",
       " 'antiqu',\n",
       " 'anim',\n",
       " 'fiction',\n",
       " 'musician',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'russian',\n",
       " 'languag',\n",
       " 'materi',\n",
       " 'fiction',\n",
       " 'austen',\n",
       " 'jane',\n",
       " 'home',\n",
       " 'and',\n",
       " 'haunt',\n",
       " 'england',\n",
       " 'austen',\n",
       " 'jane',\n",
       " 'famili',\n",
       " 'novelist',\n",
       " 'english',\n",
       " 'home',\n",
       " 'and',\n",
       " 'haunt',\n",
       " 'england',\n",
       " 'novelist',\n",
       " 'english',\n",
       " '19th',\n",
       " 'centuri',\n",
       " 'biographi',\n",
       " 'literari',\n",
       " 'landmark',\n",
       " 'england',\n",
       " 'england',\n",
       " 'in',\n",
       " 'literatur',\n",
       " 'raspberri',\n",
       " 'pi',\n",
       " 'comput',\n",
       " 'raspberri',\n",
       " 'pi',\n",
       " 'comput',\n",
       " 'program',\n",
       " 'love',\n",
       " 'nat',\n",
       " 'fiction',\n",
       " 'african',\n",
       " 'american',\n",
       " 'cowboy',\n",
       " 'west',\n",
       " 'u',\n",
       " 's',\n",
       " 'fiction',\n",
       " 'adob',\n",
       " 'wall',\n",
       " '2nd',\n",
       " 'battl',\n",
       " 'of',\n",
       " 'tex',\n",
       " 'fiction',\n",
       " 'western',\n",
       " 'stori',\n",
       " 'handicraft',\n",
       " 'recycl',\n",
       " 'wast',\n",
       " 'etc',\n",
       " 'environment',\n",
       " 'zodiac',\n",
       " 'fiction',\n",
       " 'astrolog',\n",
       " 'fiction',\n",
       " 'scienc',\n",
       " 'fiction',\n",
       " 'fantasi',\n",
       " 'fiction',\n",
       " 'young',\n",
       " 'adult',\n",
       " 'fiction',\n",
       " 'real',\n",
       " 'properti',\n",
       " 'fiction',\n",
       " 'marseill',\n",
       " 'franc',\n",
       " 'fiction',\n",
       " 'mysteri',\n",
       " 'fiction',\n",
       " 'captain',\n",
       " 'underp',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'school',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'school',\n",
       " 'princip',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'hero',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'humor',\n",
       " 'stori',\n",
       " 'webb',\n",
       " 'sheyann',\n",
       " 'childhood',\n",
       " 'and',\n",
       " 'youth',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'nelson',\n",
       " 'rachel',\n",
       " 'west',\n",
       " 'childhood',\n",
       " 'and',\n",
       " 'youth',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'civil',\n",
       " 'right',\n",
       " 'movement',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'race',\n",
       " 'relat',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'african',\n",
       " 'american',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'martin',\n",
       " 'luther',\n",
       " 'king',\n",
       " 'jr',\n",
       " 'day',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'alabama',\n",
       " 'histori',\n",
       " '20th',\n",
       " 'centuri',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'scienc',\n",
       " 'fiction',\n",
       " 'american',\n",
       " 'scienc',\n",
       " 'fiction',\n",
       " 'short',\n",
       " 'stori',\n",
       " 'famili',\n",
       " 'secret',\n",
       " 'fiction',\n",
       " 'conspiraci',\n",
       " 'fiction',\n",
       " 'sweden',\n",
       " 'fiction',\n",
       " 'suspens',\n",
       " 'fiction',\n",
       " 'wine',\n",
       " 'and',\n",
       " 'wine',\n",
       " 'make',\n",
       " 'dinosaur',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'spanish',\n",
       " 'languag',\n",
       " 'materi',\n",
       " 'spider',\n",
       " 'man',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'spider',\n",
       " 'man',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'fiction',\n",
       " 'teenag',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'racial',\n",
       " 'mix',\n",
       " 'youth',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'superhero',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'teenag',\n",
       " 'fiction',\n",
       " 'racial',\n",
       " 'mix',\n",
       " 'youth',\n",
       " 'fiction',\n",
       " 'superhero',\n",
       " 'fiction',\n",
       " 'brooklyn',\n",
       " 'new',\n",
       " 'york',\n",
       " 'n',\n",
       " 'y',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'brooklyn',\n",
       " 'new',\n",
       " 'york',\n",
       " 'n',\n",
       " 'y',\n",
       " 'fiction',\n",
       " 'slug',\n",
       " 'mollusk',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'cook',\n",
       " 'food',\n",
       " 'gastronomi',\n",
       " 'meyer',\n",
       " 'michael',\n",
       " 'j',\n",
       " 'travel',\n",
       " 'china',\n",
       " 'china',\n",
       " 'descript',\n",
       " 'and',\n",
       " 'travel',\n",
       " 'autobiographi',\n",
       " 'travel',\n",
       " 'write',\n",
       " 'school',\n",
       " 'integr',\n",
       " 'california',\n",
       " 'berkeley',\n",
       " 'pictori',\n",
       " 'work',\n",
       " 'famili',\n",
       " 'life',\n",
       " 'fiction',\n",
       " 'middl',\n",
       " 'born',\n",
       " 'children',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'school',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'theater',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'famili',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'brother',\n",
       " 'and',\n",
       " 'sister',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'humor',\n",
       " 'stori',\n",
       " 'samsung',\n",
       " 'galaxi',\n",
       " 's',\n",
       " 'smartphon',\n",
       " 'smartphon',\n",
       " 'pocket',\n",
       " 'comput',\n",
       " 'bergdorf',\n",
       " 'goodman',\n",
       " 'new',\n",
       " 'york',\n",
       " 'n',\n",
       " 'y',\n",
       " 'cook',\n",
       " 'cookbook',\n",
       " 'folklor',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'fabl',\n",
       " 'juvenil',\n",
       " 'literatur',\n",
       " 'earli',\n",
       " 'childhood',\n",
       " 'educ',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'success',\n",
       " 'in',\n",
       " 'children',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'cook',\n",
       " 'mediterranean',\n",
       " 'cookbook',\n",
       " 'cougar',\n",
       " 'jet',\n",
       " 'fighter',\n",
       " 'plane',\n",
       " 'good',\n",
       " 'and',\n",
       " 'evil',\n",
       " 'fiction',\n",
       " 'secret',\n",
       " 'societi',\n",
       " 'fiction',\n",
       " 'thriller',\n",
       " 'fiction',\n",
       " 'bourn',\n",
       " 'jason',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'fiction',\n",
       " 'intellig',\n",
       " 'offic',\n",
       " 'fiction',\n",
       " 'terrorist',\n",
       " 'fiction',\n",
       " 'spi',\n",
       " 'stori',\n",
       " 'adventur',\n",
       " 'fiction',\n",
       " 'giant',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'orphan',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'indigen',\n",
       " 'peopl',\n",
       " 'brazil',\n",
       " 'fiction',\n",
       " 'spanish',\n",
       " 'languag',\n",
       " 'noun',\n",
       " 'spanish',\n",
       " 'languag',\n",
       " 'materi',\n",
       " 'rehm',\n",
       " 'dian',\n",
       " 'rehm',\n",
       " 'john',\n",
       " 'b',\n",
       " 'health',\n",
       " 'radio',\n",
       " 'broadcast',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'biographi',\n",
       " 'widow',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'biographi',\n",
       " 'bereav',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'loss',\n",
       " 'psycholog',\n",
       " 'adjust',\n",
       " 'psycholog',\n",
       " 'parkinson',\n",
       " 'diseas',\n",
       " 'patient',\n",
       " 'famili',\n",
       " 'relationship',\n",
       " 'autobiographi',\n",
       " 'psychic',\n",
       " 'abil',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'immort',\n",
       " 'fiction',\n",
       " 'paranorm',\n",
       " 'fiction',\n",
       " 'pete',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'fictiti',\n",
       " 'charact',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'cat',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'playground',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'construct',\n",
       " 'equip',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'construct',\n",
       " 'worker',\n",
       " 'juvenil',\n",
       " 'fiction',\n",
       " 'orwel',\n",
       " 'georg',\n",
       " 'diari',\n",
       " 'citi',\n",
       " 'and',\n",
       " 'town',\n",
       " 'new',\n",
       " 'york',\n",
       " 'state',\n",
       " 'fiction',\n",
       " 'women',\n",
       " 'librarian',\n",
       " 'fiction',\n",
       " 'femal',\n",
       " 'friendship',\n",
       " 'fiction',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'relationship',\n",
       " 'fiction',\n",
       " 'good',\n",
       " 'and',\n",
       " 'evil',\n",
       " 'fiction',\n",
       " 'magic',\n",
       " 'fiction',\n",
       " 'new',\n",
       " 'york',\n",
       " 'state',\n",
       " 'fiction',\n",
       " 'romanc',\n",
       " 'fiction',\n",
       " 'paranorm',\n",
       " 'fiction',\n",
       " 'photographi',\n",
       " 'of',\n",
       " 'handicraft',\n",
       " 'advertis',\n",
       " 'photographi',\n",
       " 'photographi',\n",
       " 'digit',\n",
       " 'techniqu',\n",
       " 'male',\n",
       " 'friendship',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'social',\n",
       " 'class',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'betray',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'kabul',\n",
       " 'afghanistan',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'psychic',\n",
       " 'abil',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'teenag',\n",
       " 'girl',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'ghost',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'supernatur',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'strip',\n",
       " 'etc',\n",
       " 'comic',\n",
       " ...]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipeline.fit_transform(X = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(X, default_column = 'subjects'):\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(X[default_column]) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "#     filtered_tokens = []\n",
    "#     [filtered_tokens.append(token) if re.search('[a-zA-Z]', token) else token for token in tokens]\n",
    "    \n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        \n",
    "#     return stems\n",
    "\n",
    "# tokenize_and_stem(df_pandas_fitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "# tokenize_and_stem(df_pandas_fitter2['subjects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = df_pandas['subjects']\n",
    "\n",
    "# Define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "# #class TextTransform()\n",
    "# def tokenize_and_stem(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "#     return stems\n",
    "\n",
    "\n",
    "# def tokenize_only(text):\n",
    "#     # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "#     tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "#     filtered_tokens = []\n",
    "#     # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "#     for token in tokens:\n",
    "#         if re.search('[a-zA-Z]', token):\n",
    "#             filtered_tokens.append(token)\n",
    "#     return filtered_tokens\n",
    "\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "\n",
    "# allwords_stemmed = [tokenize_and_stem(str(i)) for i in subjects] #for each item in 'synopses', tokenize/stem\n",
    "# totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "# totalvocab_stemmed = list(more_itertools.collapse(totalvocab_stemmed)) \n",
    "\n",
    "# allwords_tokenized = [tokenize_only(str(i)) for i in subjects]\n",
    "# totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# totalvocab_tokenized = list(more_itertools.collapse(totalvocab_tokenized)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in subjects_c:\n",
    "#     allwords_stemmed = tokenize_and_stem(str(i)) #for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(str(i))\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return pd.DataFrame(self.model.predict(X))\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('cluster', ModelTransformer(KMeans_foo(3))),\n",
    "#     ('binarize', LabelBinarizer())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.loc[:,'PUBLICATIONYEAR'] = df_pandas.loc[:,'PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n",
    "# _______________________________________________________________________________\n",
    "# Functions for transforming the data\n",
    "# def processing(df):\n",
    "#     df['PUBLICATIONYEAR'] = df['PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "#     [str(item) for item in df['SUBJECTS'] if item is None]\n",
    "\n",
    "# processing(df_pandas)\n",
    "    \n",
    "    # def ext_date_fun(input, output):\n",
    "#   output = input.str.extractstr.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "  \n",
    "# def fix_none_fun(sub):\n",
    "#   [str(item) for item in sub if item is None]\n",
    "# _______________________________________________________________________________\n",
    "\n",
    "# # Attempt at putting the functions into classes\n",
    "# class PublicationYearCleaner(object):\n",
    "#   \"\"\"Preprocessing: This class cleans the Publication Year Column\"\"\"\n",
    "#   def __init__(self, data):\n",
    "#     self.raw = data\n",
    "  \n",
    "#   def ext_date(self, pub_year):\n",
    "#      pub_year = pub_year.str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n",
    "# class SubjectsCleaner(objct)\n",
    "#   def fix_none(self, string):\n",
    "#     if string is None:\n",
    "#       return ''\n",
    "#     return str(string)\n",
    "# # ________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas.loc[:,'PUBLICATIONYEAR'] = df_pandas.loc[:,'PUBLICATIONYEAR'].str.extract(r'(^|)*([0-9]{4})\\s*(|$)', expand=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000  # Number of rows to analyse\n",
    "\n",
    "# titles = df_pandas.loc[0:n,'TITLE'].values.tolist()\n",
    "# subjects = df_pandas.loc[0:n,'SUBJECTS'].values.tolist()\n",
    "# author = df_pandas.loc[0:n, 'AUTHOR'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return pd.DataFrame(self.model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMTYPE']), how='inner')\n",
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMCOLLECTION']), how='inner')\n",
    "# df_pandas = df_pandas.join(pd.get_dummies(df_pandas.loc[:,'ITEMLOCATION']), how='inner')\n",
    "# list(df_pandas.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use extend so it's a big flat list of vocab\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in df_pandas['subjects_c']:\n",
    "#     allwords_stemmed = tokenize_and_stem(str(i)) #for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(str(i))\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 13598 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              words\n",
      "allon         allon\n",
      "gabriel     gabriel\n",
      "fictiti  fictitious\n",
      "charact   character\n",
      "fiction     fiction\n"
     ]
    }
   ],
   "source": [
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 450 ms\n",
      "(1000, 737)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "vectorizer_pipe = Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=5, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3)))])\n",
    "\n",
    "%time tfidf_matrix = vectorizer_pipe.fit_transform(subjects)  # fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-838-d2a0c894f183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcluster_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3425\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cluster_pipe.fit(tfidf_matrix)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "dist = 1 - euclidean_distances(tfidf_matrix)\n",
    "\n",
    "cluster_pipe = Pipeline(steps=[('cluster', KMeans(n_clusters = 5, random_state=3425))])\n",
    "%time cluster_pipe.fit(tfidf_matrix)\n",
    "clusters = cluster_pipe.named_steps['cluster'].labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(cluster_pipe,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = cluster_pipe.named_steps['cluster'].labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = { 'title': titles, 'author': author, 'subjects': subjects_c, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(books, index = [clusters] , columns = ['title', 'author', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame.columns\n",
    "frame.cluster.value_counts() #number of books per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(ind)\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = cluster_pipe.named_steps['cluster'].cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace\n",
    "    \n",
    "    for ind in order_centroids[i, :3]: #replace 6 with n words per cluster\n",
    "        x += ' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=','\n",
    "    \n",
    "#    print(\"Cluster %d titles:\" % i, end='')\n",
    "#    for title in frame.loc[i,'title'].values.tolist():\n",
    "#        print(' %s,' % title, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os  # for os.path.basename\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# from sklearn.manifold import MDS\n",
    "\n",
    "# MDS()\n",
    "\n",
    "# # convert two components as we're plotting points in a two-dimensional plane\n",
    "# # \"precomputed\" because we provide a distance matrix\n",
    "# # we will also specify `random_state` so the plot is reproducible.\n",
    "# mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "# pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "# xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(frame.cluster)\n",
    "dummies.columns = dummies.columns.astype(str)\n",
    "list(dummies.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies.rename(index=str, columns={\"0\": \"Juvenile Literature\", \"1\": \"Music (Country)\", \"2\": \"Mystery\", \"3\": \"Music (Rock)\", \"4\": \"Comic Books\"})\n",
    "dummies.columns = [\"Drama / Film / Rock\", \"Juvinile Mystery\", \"United States / Biography\", \"Juvinile Literature / Biography\", \"Comic Books\"]\n",
    "list(dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame\n",
    "df_pandas_dummies = df_pandas.join(dummies, how='left')\n",
    "df_pandas_dummies\n",
    "list(df_pandas_dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = df_pandas.loc[(n+1):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_pandas_dummies, dummies, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "Library_Collection_Forecasting",
  "notebookId": 3995305047714984
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
